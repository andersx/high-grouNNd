#!/usr/bin/env python3
# MIT License
#
# Copyright (c) 2017 Anders Steen Christensen
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

from __future__ import print_function

import os
import numpy as np

import qml
import qml.data

from qml.kernels import laplacian_kernel
from qml.math import cho_solve
from qml.representations import get_slatm_mbtypes


def get_energies(filename):
    """ Returns a dictionary with heats of formation for each xyz-file.
    """

    f = open(filename, "r")
    lines = f.readlines()
    f.close()

    energies = dict()

    for line in lines:
        tokens = line.split()

        xyz_name = tokens[0]
        hof = float(tokens[1])
        hof2 = float(tokens[1])

        energies[xyz_name] = (hof - hof2) / 627.51

    return energies

def test_nn_bob():

    test_dir = os.path.dirname(os.path.realpath(__file__))

    # Parse file containing PBE0/def2-TZVP heats of formation and xyz filenames
    data = get_energies(test_dir + "/data/hof_qm7.txt")

    # Generate a list of qml.data.Compound() objects
    mols = []


    numbers = dict()

    for xyz_file in sorted(data.keys()):

        # Initialize the qml.data.Compound() objects
        mol = qml.data.Compound(xyz=test_dir + "/qm7/" + xyz_file)

        # Associate a property (heat of formation) with the object
        mol.properties = data[xyz_file]

        # This is a Molecular Coulomb matrix sorted by row norm
        mol.generate_eigenvalue_coulomb_matrix()
        # print(mol.representation)
        mols.append(mol)

    # Shuffle molecules
    np.random.seed(666)
    np.random.shuffle(mols)

    # Make training and test sets
    n_test  = 1000
    n_train = 1000

    training = mols[:n_train]
    test  = mols[-n_test:]

    # List of representations
    X  = np.array([mol.representation for mol in training])
    Xs = np.array([mol.representation for mol in test])


    # List of properties
    Y = np.array([mol.properties for mol in training])
    Ys = np.array([mol.properties for mol in test])

    print(X.shape)
    print(Y.shape)
    # exit()


    import torch


    dtype = torch.float
    # device = torch.device("cpu")
    device = torch.device("cuda:0") # Uncomment this to run on GPU

    N, D_in, H, D_out = n_train, X.shape[1], 64, 1

    x = torch.from_numpy(X).to(device, torch.float)
    y = torch.from_numpy(Y.reshape((N,1))).to(device, torch.float)

    xs = torch.from_numpy(Xs).to(device, torch.float)
    ys = torch.from_numpy(Ys.reshape((n_test,1))).to(device, torch.float)

    # print(x.shape)
    # print(y.shape)

    # print(x)
    # print(X)
    # # Randomly initialize weights
    # w1 = torch.randn(D_in, H, device=device, dtype=dtype)
    # w2 = torch.randn(H, D_out, device=device, dtype=dtype)

    # print(w1.shape)
    # print(w2.shape)

    # N, D_in, H, D_out = 64, 1000, 100, 10

# # Create random input and output data
#     x = torch.randn(N, D_in, device=device, dtype=dtype)
#     y = torch.randn(N, D_out, device=device, dtype=dtype)

# Randomly initialize weights
    w1 = torch.randn(D_in, H, device=device, dtype=dtype)
    w2 = torch.randn(H, D_out, device=device, dtype=dtype)


    learning_rate = 1e-8
    for t in range(5000):
        # Forward pass: compute predicted y
        h = x.mm(w1)
        h_relu = h.clamp(min=0)
        y_pred = h_relu.mm(w2)

        # Compute and print loss
        loss = (y_pred - y).pow(2).sum().item()

    
        hval = xs.mm(w1)
        hval_relu = hval.clamp(min=0)
        yval_pred = hval_relu.mm(w2)

        loss2 = (yval_pred - ys).pow(2).sum().item()
        print(t, loss / n_train *627.51, loss2 / n_test *627.51)

        # Backprop to compute gradients of w1 and w2 with respect to loss
        grad_y_pred = 2.0 * (y_pred - y)
        grad_w2 = h_relu.t().mm(grad_y_pred)
        grad_h_relu = grad_y_pred.mm(w2.t())
        grad_h = grad_h_relu.clone()
        grad_h[h < 0] = 0
        grad_w1 = x.t().mm(grad_h)

        # Update weights using gradient descent
        w1 -= learning_rate * grad_w1
        w2 -= learning_rate * grad_w2






















if __name__ == "__main__":

    test_nn_bob()
